{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第3回レポート課題\n",
    "\n",
    "学籍番号：XX-YYY\n",
    "氏名：ZZZZ ZZZZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR10という画像データセットに対して、50000枚の学習用画像を用いて独自のニューラルネットワークを学習し、学習したニューラルネットワークを10000枚のテスト用画像に適用して、その結果を報告せよ。<font color=\"red\">特に、下記の点に従って、本レポートを作成すること。\n",
    "    \n",
    "- 下記に指定されたフォーマットで、ニューラルネットワークの学習・テスト用コードを実装すること\n",
    "\n",
    "\n",
    "- 下記に指定されたフォーマットで各処理を数学的に記述すること\n",
    "\n",
    "</font>\n",
    "\n",
    "なお、CIFAR10の詳細情報に関しては、下記の公式ページを参照するか、もしくはその他多くのWebページを調べれば分かる。\n",
    "\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データの読み込みと説明\n",
    "\n",
    "### 下記の要領で、4つのnumpyデータを読み込め。（5点）\n",
    "\n",
    "- `train_data.npy`：50000枚の学習用画像が格納されている本データを読み込み、`x_train`という変数に格納せよ。各画像は、サイズが縦32ピクセル、横32ピクセルで、3チャンネルのRGBデータである。<font color=\"red\">特に、`x_train`は、行数が50000、列数が3072の行列（numpy array）とする。</font>行数は学習用画像の数で明らかであるが、列数は「RGBの3つの値で表された32$\\times$32個のピクセル」を **1行に整形**することによる（つまり、$3072 = 32 \\times 32 \\times 3$）。\n",
    "\n",
    "\n",
    "- `train_labels.npy`：50000枚の学習用画像のラベルが格納されいる本データを読み込み、`y_train`という変数に格納せよ。<font color=\"red\">特に、`y_train`は、行数が50000、列数が1の行列（numpy array）とする。</font>つまり、各行が、各学習用画像が属するクラスのIDを表している。\n",
    "\n",
    "\n",
    "- `test_data.npy`：10000枚のテスト用画像が格納されている本データを読み込み、`x_test`という変数に格納せよ。学習用画像と同様、各画像は、サイズが32x32で、3チャンネルのRGBデータである。<font color=\"red\">上述の`x_train`と同様の仕様に従い、`x_test`は、行数が10000、列数が3072の行列（numpy array）とする。</font>\n",
    "\n",
    "\n",
    "- `test_labels.npy`：10000枚のテスト用画像のラベルが格納されている本データを読み込み、`y_test`という変数に格納せよ。<font color=\"red\">上述の`y_train`と同様の仕様に従い、`y_test`は、行数が10000、列数が1の行列（numpy array）とする。</font>\n",
    "\n",
    "\n",
    "<font color=\"blue\">「どの画像が学習用で、どの画像がテスト用か」という学習用画像とテスト用画像の分割は、絶対に変更しないこと！ 変更した場合は、大幅に減点する。</font>また、データを読み込んだ後で、以降のニューラルネットワークの学習、テストで必要な**データの正規化**を行ってもらって構わない。この正規化は採点外である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コードを記述するセル\n",
    "import numpy as np\n",
    "\n",
    "# np.reshapeで50000x3072のnumpy arrayに変更.\n",
    "x_train = np.reshape(np.load('train_data.npy'), (50000, 3072))\n",
    "\n",
    "# np.reshapeで50000x1のnumpy arrayに変更.\n",
    "y_train = np.reshape(np.load('train_labels.npy'), (50000, 1))\n",
    "\n",
    "# np.reshapeで10000x3072のnumpy arrayに変更.\n",
    "x_test = np.reshape(np.load('test_data.npy'), (10000, 3072))\n",
    "\n",
    "# np.reshapeで10000x1のnumpy arrayに変更.\n",
    "y_test = np.reshape(np.load('test_labels.npy'), (10000, 1))\n",
    "\n",
    "# 以下正規化\n",
    "x_train = x_train.astype('float32') # float32型に変換\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255 # RGB値を[0,255]から[0.0,1.0へ]\n",
    "x_test /= 255\n",
    "\n",
    "import keras\n",
    "\n",
    "# 今回の場合ではIDが10個あるので, 1の場合なら[1,0,0,0,0,0,0,0,0,0]\n",
    "# 2なら[0,1,0,0,0,0,0,0,0,0]という風に表している.\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# 以下, 確認用\n",
    "# print(x_train)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上記で読み込んだデータのうち、$P=50000$個の学習用画像を$\\left\\{ (\\mathbf{x}_p, y_p) \\right\\}_{p=1}^{P}$としたとき、$\\mathbf{x}_p$と$y_p$が、何を表しているか**数学的に**説明せよ。（5点）\n",
    "\n",
    "つまり、\n",
    "\n",
    "- $\\mathbf{x}_p$は、$p$番目の学習用画像の特徴をどのように表現しているベクトルか\n",
    "\n",
    "\n",
    "- $y_p$は、$p$番目の学習用画像のラベルをどのように表現しているか\n",
    "\n",
    "について説明せよ。詳細な説明は不要で、$\\mathbf{x}_p$、$y_p$をそれぞれ、1, 2文で**簡潔かつ直感的に**説明することが望ましい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{x}_p$は$p$番目の学習用画像のRGBの3つの値で表された32 × 32個のピクセルを1行で表している. \n",
    "\n",
    "$y_p$は$p$番目の学習用画像のラベル0~9を1行で表している."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 学習するモデルの実装と説明\n",
    "\n",
    "### `x_train`と`y_train`を用いて学習する複数の層からなるニューラルネットワークを実装せよ。（5点）\n",
    "\n",
    "層数、ユニット数、活性化関数などのニューラルネットワークの構造は、各自の任意とする。また、自分でニューラルネットワークを実装するのではなく、kerasというディープラーニングライブラリを用いることをお勧めする。kerasのインストール方法に関しては、下記を参照するとよい。\n",
    "\n",
    "https://www.info.kindai.ac.jp/~shirahama/courses/ml/2018/slides/installation_guide_mac.pdf\n",
    "\n",
    "もちろん、自分で実装したり、keras以外の別のライブラリを使用してもらっても構わない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# 学習のためのモデル\n",
    "model = Sequential()\n",
    "# 全結合層(3072->1024->512->256->10)\n",
    "# 活性化関数(ReLu関数)\n",
    "model.add(Dense(1024, activation='relu', input_dim=3072))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# 活性化関数(softmax関数) \n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上記で実装したニューラルネットワークを**数学的に**説明せよ。（5点）\n",
    "\n",
    "つまり、それぞれの層の各ユニットで、どのような計算が行われているのか式で示せ。ただし、以下の点に留意すること。\n",
    "\n",
    "- <font color=\"red\">1層目のユニットの入力は、$\\mathbf{x}_p$とする。</font>\n",
    "\n",
    "\n",
    "- 1層目のユニットの入力以外は、自由に記号を定義してもらって構わない。ただし、定義した記号は、必ず説明すること。\n",
    "\n",
    "\n",
    "- 式で記述する層は、入力層、基底関数の層（全結合層）、活性化関数の層、出力層だけでよい。授業では解説していないDropout、バッチ正規化などの層を用いる場合は、それらの層の概念的な説明のみでよい。\n",
    "\n",
    "\n",
    "- 例えば、同じような基底関数の層を複数使用したとすると、同じような計算を繰り返して書く必要が出てくる。その場合は、例えば「この層のユニットの計算はX層目と同様」というように省略してもらって構わない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 入力層\n",
    "入力層から隠れ層への伝達を考える.\n",
    "\n",
    "重みを$w$, スカラーを$b$, 入力値を$x_p$とすると, 隠れ層のノード1つに対しての式$u$は以下のようになる.\n",
    "\n",
    "$$\n",
    "u = w*x_p + b\n",
    "$$\n",
    "\n",
    "# 基底関数の層\n",
    "\n",
    "今回基底関数の層にはDenseを用いており, 入力層から受け取った$u$と活性化関数$a$を用いてあらわすと以下の式$v$のようになる.\n",
    "\n",
    "$$\n",
    "v = a(w*u + b)\n",
    "$$\n",
    "\n",
    "また, Dropoutに関しては引数の割合分ランダムに入力ユニットを0にするというものである.\n",
    "\n",
    "# 活性化関数の層\n",
    "今回活性化関数にはReLUとSoftmaxを用いた.\n",
    "\n",
    "## ReLU\n",
    "xは隠れ層からの値としてReLUは式$r$のようにあらわせれる.\n",
    "$$\n",
    "r=max(0,x)\n",
    "$$\n",
    "\n",
    "この$max$という関数は上手く分類されていたら0そうでなければ$x$を返すような関数である.\n",
    "今回では0以下の数値は破棄して, それ以外はそのままとしている.\n",
    "\n",
    "## Softmax\n",
    "出力層の数を$n$, $k$番目の出力を$y_k$, 隠れ層からの値を$a_k$とすると以下のようになる.\n",
    "\n",
    "$$\n",
    "y_k = \\frac{exp(a_k)}{\\sum_{i=1}^{n}exp(a_i)}\n",
    "$$\n",
    "\n",
    "# 出力層\n",
    "隠れ層からの入力値を$a$, 活性化関数はsoftmax, 出力を$y$とすると式は以下のようになる.\n",
    "\n",
    "$$\n",
    "y = softmax(a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 学習プロセスの実装と説明\n",
    "\n",
    "### `x_train`、`y_train`を用いて、上記で定義したニューラルネットワークを学習するコードを実装せよ。（5点）\n",
    "\n",
    "kerasなどのディープラーニングライブラリを用いれば、数行で書けると思われる。もちろん、自分で学習用のコードを実装しても構わないが、かなり長いコードになるはずなのでお勧めはしない。\n",
    "\n",
    "また、学習の設定（例えば、オプティマイザー、エポック数、バッチサイズなど）をコメントとして記述すること。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 1024)              3146752   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 3,805,450\n",
      "Trainable params: 3,805,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "50000/50000 [==============================] - 9s 172us/step - loss: 1.9573 - accuracy: 0.2870 - val_loss: 1.7502 - val_accuracy: 0.3714\n",
      "Epoch 2/30\n",
      "50000/50000 [==============================] - 8s 166us/step - loss: 1.7671 - accuracy: 0.3656 - val_loss: 1.6617 - val_accuracy: 0.4060\n",
      "Epoch 3/30\n",
      "50000/50000 [==============================] - 8s 168us/step - loss: 1.6872 - accuracy: 0.4004 - val_loss: 1.6124 - val_accuracy: 0.4257\n",
      "Epoch 4/30\n",
      "50000/50000 [==============================] - 8s 166us/step - loss: 1.6344 - accuracy: 0.4175 - val_loss: 1.5399 - val_accuracy: 0.4494\n",
      "Epoch 5/30\n",
      "50000/50000 [==============================] - 8s 166us/step - loss: 1.5907 - accuracy: 0.4333 - val_loss: 1.4984 - val_accuracy: 0.4691\n",
      "Epoch 6/30\n",
      "50000/50000 [==============================] - 8s 166us/step - loss: 1.5499 - accuracy: 0.4479 - val_loss: 1.5138 - val_accuracy: 0.4573\n",
      "Epoch 7/30\n",
      "50000/50000 [==============================] - 8s 165us/step - loss: 1.5196 - accuracy: 0.4582 - val_loss: 1.4737 - val_accuracy: 0.4681\n",
      "Epoch 8/30\n",
      "50000/50000 [==============================] - 8s 168us/step - loss: 1.4971 - accuracy: 0.4666 - val_loss: 1.4291 - val_accuracy: 0.4858\n",
      "Epoch 9/30\n",
      "50000/50000 [==============================] - 9s 171us/step - loss: 1.4604 - accuracy: 0.4779 - val_loss: 1.4272 - val_accuracy: 0.4862\n",
      "Epoch 10/30\n",
      "50000/50000 [==============================] - 8s 166us/step - loss: 1.4414 - accuracy: 0.4846 - val_loss: 1.4159 - val_accuracy: 0.4975\n",
      "Epoch 11/30\n",
      "50000/50000 [==============================] - 8s 166us/step - loss: 1.4171 - accuracy: 0.4930 - val_loss: 1.4066 - val_accuracy: 0.4902\n",
      "Epoch 12/30\n",
      "50000/50000 [==============================] - 8s 167us/step - loss: 1.3964 - accuracy: 0.5025 - val_loss: 1.3854 - val_accuracy: 0.5013\n",
      "Epoch 13/30\n",
      "50000/50000 [==============================] - 8s 169us/step - loss: 1.3777 - accuracy: 0.5082 - val_loss: 1.3869 - val_accuracy: 0.5078\n",
      "Epoch 14/30\n",
      "50000/50000 [==============================] - 8s 169us/step - loss: 1.3602 - accuracy: 0.5143 - val_loss: 1.3731 - val_accuracy: 0.5115\n",
      "Epoch 15/30\n",
      "50000/50000 [==============================] - 8s 167us/step - loss: 1.3432 - accuracy: 0.5213 - val_loss: 1.3383 - val_accuracy: 0.5245\n",
      "Epoch 16/30\n",
      "50000/50000 [==============================] - 9s 172us/step - loss: 1.3258 - accuracy: 0.5252 - val_loss: 1.3666 - val_accuracy: 0.5197\n",
      "Epoch 17/30\n",
      "50000/50000 [==============================] - 8s 168us/step - loss: 1.3081 - accuracy: 0.5323 - val_loss: 1.3336 - val_accuracy: 0.5263\n",
      "Epoch 18/30\n",
      "50000/50000 [==============================] - 8s 166us/step - loss: 1.3034 - accuracy: 0.5346 - val_loss: 1.3381 - val_accuracy: 0.5223\n",
      "Epoch 19/30\n",
      "50000/50000 [==============================] - 8s 165us/step - loss: 1.2797 - accuracy: 0.5429 - val_loss: 1.3265 - val_accuracy: 0.5246\n",
      "Epoch 20/30\n",
      "50000/50000 [==============================] - 9s 174us/step - loss: 1.2698 - accuracy: 0.5464 - val_loss: 1.3280 - val_accuracy: 0.5337\n",
      "Epoch 21/30\n",
      "50000/50000 [==============================] - 9s 180us/step - loss: 1.2523 - accuracy: 0.5524 - val_loss: 1.3226 - val_accuracy: 0.5225\n",
      "Epoch 22/30\n",
      "50000/50000 [==============================] - 10s 195us/step - loss: 1.2373 - accuracy: 0.5581 - val_loss: 1.3214 - val_accuracy: 0.5328\n",
      "Epoch 23/30\n",
      "50000/50000 [==============================] - 9s 180us/step - loss: 1.2281 - accuracy: 0.5599 - val_loss: 1.3214 - val_accuracy: 0.5356\n",
      "Epoch 24/30\n",
      "50000/50000 [==============================] - 9s 174us/step - loss: 1.2104 - accuracy: 0.5669 - val_loss: 1.3053 - val_accuracy: 0.5327\n",
      "Epoch 25/30\n",
      "50000/50000 [==============================] - 8s 169us/step - loss: 1.1972 - accuracy: 0.5716 - val_loss: 1.2988 - val_accuracy: 0.5354\n",
      "Epoch 26/30\n",
      "50000/50000 [==============================] - 8s 169us/step - loss: 1.1813 - accuracy: 0.5770 - val_loss: 1.2973 - val_accuracy: 0.5406\n",
      "Epoch 27/30\n",
      "50000/50000 [==============================] - 8s 167us/step - loss: 1.1705 - accuracy: 0.5813 - val_loss: 1.2897 - val_accuracy: 0.5398\n",
      "Epoch 28/30\n",
      "50000/50000 [==============================] - 8s 167us/step - loss: 1.1566 - accuracy: 0.5861 - val_loss: 1.2743 - val_accuracy: 0.5495\n",
      "Epoch 29/30\n",
      "50000/50000 [==============================] - 9s 170us/step - loss: 1.1400 - accuracy: 0.5917 - val_loss: 1.2819 - val_accuracy: 0.5470\n",
      "Epoch 30/30\n",
      "50000/50000 [==============================] - 9s 171us/step - loss: 1.1361 - accuracy: 0.5948 - val_loss: 1.2941 - val_accuracy: 0.5421\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "model.summary()\n",
    "# 確率的勾配降下法\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,  # 画像とラベルデータ\n",
    "                    batch_size=128, # batch_sizeずつ学習\n",
    "                    epochs=30,     # エポック数の指定\n",
    "                    verbose=1,         # ログ出力の指定. 0だとログが出ない\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上記のニューラルネットワークの学習で用いたコスト関数（損失関数）を数学的に説明せよ。（5点）\n",
    "\n",
    "以下の点に留意すること。\n",
    "\n",
    "- <font color=\"red\">上で定義した学習例の集合$\\left\\{ (\\mathbf{x}_p, y_p) \\right\\}_{p=1}^{P}$を用いること。</font>\n",
    "\n",
    "\n",
    "- 説明対象は、最上位層におけるコスト関数である。つまり、コスト関数は、$p$番目の学習用画像の特徴$\\mathbf{x}_p$に対するニューラルネットワークの出力（**自分で定義する**）と、$p$番目の学習用画像のラベル$y_p$を比較するはずである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コスト関数を$L$, ニューラルネットワークの出力を$y$, $y_p'$は$y_p$をonehot表現したものとすると, 式は以下のようになる.\n",
    "\n",
    "$$\n",
    "L = -\\sum_{p=1}^{p} y_p'*log(y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. テストプロセスの実装と説明\n",
    "\n",
    "### `x_test`と`y_test`を用いて学習したニューラルネットワークをテストするコードを実装せよ。（5点）\n",
    "\n",
    "特に、テスト結果として<font color=\"red\">精度（Accuracy）</font>を出力せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.294090431022644\n",
      "Test accuracy: 0.5421000123023987\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上記のテストにおける精度の計算方法を数学的に説明せよ。（5点）\n",
    "\n",
    "以下の点に留意すること。\n",
    "\n",
    "- <font color=\"red\">Q=10000個のテスト用画像$\\left\\{ (\\mathbf{x}_q, y_q) \\right\\}_{q=1}^{Q}$とすること\n",
    "    \n",
    "    \n",
    "- $q$番目のテスト用画像に対する学習済みのニューラルネットワークの出力（**自分で定義する**)と、$q$番目のテスト用画像のラベル$y_q$を比較するはずである。学習済みのニューラルネットワークの出力は、上記の学習プロセスで定義したものとよく似てくるはずである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークの出力を$y$とする.\n",
    "\n",
    "$y$と$y_q$を比べたときに, 予測があっていれば$y=y_q$なのでこれを$T$とする.\n",
    "\n",
    "これ以外の事象を$N$とする.\n",
    "\n",
    "以上により精度は以下のような式になる.\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{T}{T+N}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">特典：精度が高い上位5名は、10点加点する！</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
