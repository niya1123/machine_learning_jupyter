{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第3回レポート課題\n",
    "\n",
    "学籍番号：XX-YYY\n",
    "氏名：ZZZZ ZZZZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR10という画像データセットに対して、50000枚の学習用画像を用いて独自のニューラルネットワークを学習し、学習したニューラルネットワークを10000枚のテスト用画像に適用して、その結果を報告せよ。<font color=\"red\">特に、下記の点に従って、本レポートを作成すること。\n",
    "    \n",
    "- 下記に指定されたフォーマットで、ニューラルネットワークの学習・テスト用コードを実装すること\n",
    "\n",
    "\n",
    "- 下記に指定されたフォーマットで各処理を数学的に記述すること\n",
    "\n",
    "</font>\n",
    "\n",
    "なお、CIFAR10の詳細情報に関しては、下記の公式ページを参照するか、もしくはその他多くのWebページを調べれば分かる。\n",
    "\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データの読み込みと説明\n",
    "\n",
    "### 下記の要領で、4つのnumpyデータを読み込め。（5点）\n",
    "\n",
    "- `train_data.npy`：50000枚の学習用画像が格納されている本データを読み込み、`x_train`という変数に格納せよ。各画像は、サイズが縦32ピクセル、横32ピクセルで、3チャンネルのRGBデータである。<font color=\"red\">特に、`x_train`は、行数が50000、列数が3072の行列（numpy array）とする。</font>行数は学習用画像の数で明らかであるが、列数は「RGBの3つの値で表された32$\\times$32個のピクセル」を **1行に整形**することによる（つまり、$3072 = 32 \\times 32 \\times 3$）。\n",
    "\n",
    "\n",
    "- `train_labels.npy`：50000枚の学習用画像のラベルが格納されいる本データを読み込み、`y_train`という変数に格納せよ。<font color=\"red\">特に、`y_train`は、行数が50000、列数が1の行列（numpy array）とする。</font>つまり、各行が、各学習用画像が属するクラスのIDを表している。\n",
    "\n",
    "\n",
    "- `test_data.npy`：10000枚のテスト用画像が格納されている本データを読み込み、`x_test`という変数に格納せよ。学習用画像と同様、各画像は、サイズが32x32で、3チャンネルのRGBデータである。<font color=\"red\">上述の`x_train`と同様の仕様に従い、`x_test`は、行数が10000、列数が3072の行列（numpy array）とする。</font>\n",
    "\n",
    "\n",
    "- `test_labels.npy`：10000枚のテスト用画像のラベルが格納されている本データを読み込み、`y_test`という変数に格納せよ。<font color=\"red\">上述の`y_train`と同様の仕様に従い、`y_test`は、行数が10000、列数が1の行列（numpy array）とする。</font>\n",
    "\n",
    "\n",
    "<font color=\"blue\">「どの画像が学習用で、どの画像がテスト用か」という学習用画像とテスト用画像の分割は、絶対に変更しないこと！ 変更した場合は、大幅に減点する。</font>また、データを読み込んだ後で、以降のニューラルネットワークの学習、テストで必要な**データの正規化**を行ってもらって構わない。この正規化は採点外である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コードを記述するセル\n",
    "import numpy as np\n",
    "\n",
    "# np.reshapeで50000x3072のnumpy arrayに変更.\n",
    "x_train = np.reshape(np.load('train_data.npy'), (50000, 3072))\n",
    "\n",
    "# np.reshapeで50000x1のnumpy arrayに変更.\n",
    "y_train = np.reshape(np.load('train_labels.npy'), (50000, 1))\n",
    "\n",
    "# np.reshapeで10000x3072のnumpy arrayに変更.\n",
    "x_test = np.reshape(np.load('test_data.npy'), (10000, 3072))\n",
    "\n",
    "# np.reshapeで10000x1のnumpy arrayに変更.\n",
    "y_test = np.reshape(np.load('test_labels.npy'), (10000, 1))\n",
    "\n",
    "# 以下正規化\n",
    "x_train = x_train.astype('float32') # float32型に変換\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255 # RGB値を[0,255]から[0.0,1.0へ]\n",
    "x_test /= 255\n",
    "\n",
    "import keras\n",
    "\n",
    "# 今回の場合ではIDが10個あるので, 1の場合なら[1,0,0,0,0,0,0,0,0,0]\n",
    "# 2なら[0,1,0,0,0,0,0,0,0,0]という風に表している.\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# 以下, 確認用\n",
    "# print(x_train)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上記で読み込んだデータのうち、$P=50000$個の学習用画像を$\\left\\{ (\\mathbf{x}_p, y_p) \\right\\}_{p=1}^{P}$としたとき、$\\mathbf{x}_p$と$y_p$が、何を表しているか**数学的に**説明せよ。（5点）\n",
    "\n",
    "つまり、\n",
    "\n",
    "- $\\mathbf{x}_p$は、$p$番目の学習用画像の特徴をどのように表現しているベクトルか\n",
    "\n",
    "\n",
    "- $y_p$は、$p$番目の学習用画像のラベルをどのように表現しているか\n",
    "\n",
    "について説明せよ。詳細な説明は不要で、$\\mathbf{x}_p$、$y_p$をそれぞれ、1, 2文で**簡潔かつ直感的に**説明することが望ましい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{x}_p$は$p$番目の学習用画像のRGBの3つの値で表された32 × 32個のピクセルを1行で表している. \n",
    "\n",
    "$y_p$は$p$番目の学習用画像のラベル0~9を1行で表している."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 学習するモデルの実装と説明\n",
    "\n",
    "### `x_train`と`y_train`を用いて学習する複数の層からなるニューラルネットワークを実装せよ。（5点）\n",
    "\n",
    "層数、ユニット数、活性化関数などのニューラルネットワークの構造は、各自の任意とする。また、自分でニューラルネットワークを実装するのではなく、kerasというディープラーニングライブラリを用いることをお勧めする。kerasのインストール方法に関しては、下記を参照するとよい。\n",
    "\n",
    "https://www.info.kindai.ac.jp/~shirahama/courses/ml/2018/slides/installation_guide_mac.pdf\n",
    "\n",
    "もちろん、自分で実装したり、keras以外の別のライブラリを使用してもらっても構わない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# 学習のためのモデル\n",
    "model = Sequential()\n",
    "# 全結合層(3072->1024->512->256->10)\n",
    "# 活性化関数(ReLu関数)\n",
    "model.add(Dense(1024, activation='relu', input_dim=3072))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# 活性化関数(softmax関数) \n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上記で実装したニューラルネットワークを**数学的に**説明せよ。（5点）\n",
    "\n",
    "つまり、それぞれの層の各ユニットで、どのような計算が行われているのか式で示せ。ただし、以下の点に留意すること。\n",
    "\n",
    "- <font color=\"red\">1層目のユニットの入力は、$\\mathbf{x}_p$とする。</font>\n",
    "\n",
    "\n",
    "- 1層目のユニットの入力以外は、自由に記号を定義してもらって構わない。ただし、定義した記号は、必ず説明すること。\n",
    "\n",
    "\n",
    "- 式で記述する層は、入力層、基底関数の層（全結合層）、活性化関数の層、出力層だけでよい。授業では解説していないDropout、バッチ正規化などの層を用いる場合は、それらの層の概念的な説明のみでよい。\n",
    "\n",
    "\n",
    "- 例えば、同じような基底関数の層を複数使用したとすると、同じような計算を繰り返して書く必要が出てくる。その場合は、例えば「この層のユニットの計算はX層目と同様」というように省略してもらって構わない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 入力層\n",
    "入力層から隠れ層への伝達を考える.\n",
    "\n",
    "重みを$w$, スカラーを$b$, 入力値を$x_p$とすると, 隠れ層のノード1つに対しての式$u$は以下のようになる.\n",
    "\n",
    "$$\n",
    "u = w*x_p + b\n",
    "$$\n",
    "\n",
    "# 基底関数の層\n",
    "\n",
    "今回基底関数の層にはDenseを用いており, 入力層から受け取った$u$と活性化関数$a$を用いてあらわすと以下の式$v$のようになる.\n",
    "\n",
    "$$\n",
    "v = a(w*u + b)\n",
    "$$\n",
    "\n",
    "また, Dropoutに関しては引数の割合分ランダムに入力ユニットを0にするというものである.\n",
    "\n",
    "# 活性化関数の層\n",
    "今回活性化関数にはReLUとSoftmaxを用いた.\n",
    "\n",
    "## ReLU\n",
    "xは隠れ層からの値としてReLUは式$r$のようにあらわせれる.\n",
    "$$\n",
    "r=max(0,x)\n",
    "$$\n",
    "\n",
    "この$max$という関数は上手く分類されていたら0そうでなければ$x$を返すような関数である.\n",
    "今回では0以下の数値は破棄して, それ以外はそのままとしている.\n",
    "\n",
    "## Softmax\n",
    "出力層の数を$n$, $k$番目の出力を$y_k$, 隠れ層からの値を$a_k$とすると以下のようになる.\n",
    "\n",
    "$$\n",
    "y_k = \\frac{exp(a_k)}{\\sum_{i=1}^{n}exp(a_i)}\n",
    "$$\n",
    "\n",
    "# 出力層\n",
    "隠れ層からの入力値を$a$, 活性化関数はsoftmax, 出力を$y$とすると式は以下のようになる.\n",
    "\n",
    "$$\n",
    "y = softmax(a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 学習プロセスの実装と説明\n",
    "\n",
    "### `x_train`、`y_train`を用いて、上記で定義したニューラルネットワークを学習するコードを実装せよ。（5点）\n",
    "\n",
    "kerasなどのディープラーニングライブラリを用いれば、数行で書けると思われる。もちろん、自分で学習用のコードを実装しても構わないが、かなり長いコードになるはずなのでお勧めはしない。\n",
    "\n",
    "また、学習の設定（例えば、オプティマイザー、エポック数、バッチサイズなど）をコメントとして記述すること。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 1024)              3146752   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 3,805,450\n",
      "Trainable params: 3,805,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "50000/50000 [==============================] - 28s 569us/step - loss: 1.9533 - accuracy: 0.2833 - val_loss: 1.7731 - val_accuracy: 0.3665\n",
      "Epoch 2/30\n",
      "50000/50000 [==============================] - 28s 554us/step - loss: 1.7638 - accuracy: 0.3643 - val_loss: 1.6934 - val_accuracy: 0.3842\n",
      "Epoch 3/30\n",
      "50000/50000 [==============================] - 26s 520us/step - loss: 1.6847 - accuracy: 0.3974 - val_loss: 1.6031 - val_accuracy: 0.4273\n",
      "Epoch 4/30\n",
      "50000/50000 [==============================] - 25s 496us/step - loss: 1.6297 - accuracy: 0.4188 - val_loss: 1.5516 - val_accuracy: 0.4473\n",
      "Epoch 5/30\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 1.5918 - accuracy: 0.4315 - val_loss: 1.5179 - val_accuracy: 0.4569\n",
      "Epoch 6/30\n",
      "50000/50000 [==============================] - 24s 472us/step - loss: 1.5529 - accuracy: 0.4441 - val_loss: 1.5131 - val_accuracy: 0.4567\n",
      "Epoch 7/30\n",
      "50000/50000 [==============================] - 25s 491us/step - loss: 1.5179 - accuracy: 0.4582 - val_loss: 1.4629 - val_accuracy: 0.4765\n",
      "Epoch 8/30\n",
      "50000/50000 [==============================] - 25s 495us/step - loss: 1.4906 - accuracy: 0.4676 - val_loss: 1.4727 - val_accuracy: 0.4608\n",
      "Epoch 9/30\n",
      "50000/50000 [==============================] - 24s 487us/step - loss: 1.4640 - accuracy: 0.4758 - val_loss: 1.4610 - val_accuracy: 0.4738\n",
      "Epoch 10/30\n",
      "50000/50000 [==============================] - 28s 567us/step - loss: 1.4419 - accuracy: 0.4856 - val_loss: 1.4035 - val_accuracy: 0.4909\n",
      "Epoch 11/30\n",
      "50000/50000 [==============================] - 30s 594us/step - loss: 1.4146 - accuracy: 0.4927 - val_loss: 1.4374 - val_accuracy: 0.4757\n",
      "Epoch 12/30\n",
      "50000/50000 [==============================] - 34s 685us/step - loss: 1.4013 - accuracy: 0.4977 - val_loss: 1.3658 - val_accuracy: 0.5150\n",
      "Epoch 13/30\n",
      "50000/50000 [==============================] - 29s 590us/step - loss: 1.3781 - accuracy: 0.5059 - val_loss: 1.3936 - val_accuracy: 0.4996\n",
      "Epoch 14/30\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 1.3611 - accuracy: 0.5135 - val_loss: 1.3813 - val_accuracy: 0.5099\n",
      "Epoch 15/30\n",
      "50000/50000 [==============================] - 25s 491us/step - loss: 1.3388 - accuracy: 0.5241 - val_loss: 1.3515 - val_accuracy: 0.5243\n",
      "Epoch 16/30\n",
      "50000/50000 [==============================] - 25s 495us/step - loss: 1.3268 - accuracy: 0.5255 - val_loss: 1.3378 - val_accuracy: 0.5238\n",
      "Epoch 17/30\n",
      "50000/50000 [==============================] - 25s 491us/step - loss: 1.3070 - accuracy: 0.5341 - val_loss: 1.3414 - val_accuracy: 0.5215\n",
      "Epoch 18/30\n",
      "50000/50000 [==============================] - 24s 488us/step - loss: 1.2920 - accuracy: 0.5380 - val_loss: 1.3228 - val_accuracy: 0.5302\n",
      "Epoch 19/30\n",
      "50000/50000 [==============================] - 27s 533us/step - loss: 1.2789 - accuracy: 0.5425 - val_loss: 1.3147 - val_accuracy: 0.5302\n",
      "Epoch 20/30\n",
      "50000/50000 [==============================] - 26s 526us/step - loss: 1.2671 - accuracy: 0.5464 - val_loss: 1.3074 - val_accuracy: 0.5302\n",
      "Epoch 21/30\n",
      "50000/50000 [==============================] - 25s 494us/step - loss: 1.2473 - accuracy: 0.5566 - val_loss: 1.3188 - val_accuracy: 0.5309\n",
      "Epoch 22/30\n",
      "50000/50000 [==============================] - 25s 494us/step - loss: 1.2358 - accuracy: 0.5596 - val_loss: 1.3190 - val_accuracy: 0.5260\n",
      "Epoch 23/30\n",
      "50000/50000 [==============================] - 25s 507us/step - loss: 1.2221 - accuracy: 0.5618 - val_loss: 1.3191 - val_accuracy: 0.5301\n",
      "Epoch 24/30\n",
      "50000/50000 [==============================] - 26s 513us/step - loss: 1.2075 - accuracy: 0.5670 - val_loss: 1.2813 - val_accuracy: 0.5478\n",
      "Epoch 25/30\n",
      "50000/50000 [==============================] - 25s 506us/step - loss: 1.1976 - accuracy: 0.5723 - val_loss: 1.2882 - val_accuracy: 0.5447\n",
      "Epoch 26/30\n",
      "50000/50000 [==============================] - 30s 591us/step - loss: 1.1849 - accuracy: 0.5768 - val_loss: 1.2884 - val_accuracy: 0.5408\n",
      "Epoch 27/30\n",
      "50000/50000 [==============================] - 27s 537us/step - loss: 1.1729 - accuracy: 0.5809 - val_loss: 1.3229 - val_accuracy: 0.5360\n",
      "Epoch 28/30\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 1.1604 - accuracy: 0.5853 - val_loss: 1.2955 - val_accuracy: 0.5446\n",
      "Epoch 29/30\n",
      "50000/50000 [==============================] - 24s 487us/step - loss: 1.1461 - accuracy: 0.5902 - val_loss: 1.2725 - val_accuracy: 0.5495\n",
      "Epoch 30/30\n",
      "50000/50000 [==============================] - 24s 477us/step - loss: 1.1357 - accuracy: 0.5919 - val_loss: 1.2833 - val_accuracy: 0.5441\n"
     ]
    }
   ],
   "source": [
    "# from keras.optimizers import RMSprop\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model.summary()\n",
    "# 確率的勾配降下法\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=RMSprop(),\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,  # 画像とラベルデータ\n",
    "                    batch_size=128, # batch_sizeずつ学習\n",
    "                    epochs=30,     # エポック数の指定\n",
    "                    verbose=1,         # ログ出力の指定. 0だとログが出ない\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上記のニューラルネットワークの学習で用いたコスト関数（損失関数）を数学的に説明せよ。（5点）\n",
    "\n",
    "以下の点に留意すること。\n",
    "\n",
    "- <font color=\"red\">上で定義した学習例の集合$\\left\\{ (\\mathbf{x}_p, y_p) \\right\\}_{p=1}^{P}$を用いること。</font>\n",
    "\n",
    "\n",
    "- 説明対象は、最上位層におけるコスト関数である。つまり、コスト関数は、$p$番目の学習用画像の特徴$\\mathbf{x}_p$に対するニューラルネットワークの出力（**自分で定義する**）と、$p$番目の学習用画像のラベル$y_p$を比較するはずである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コスト関数を$L$, ニューラルネットワークの出力を$y$, $y_p'$は$y_p$をonehot表現したものとすると, 式は以下のようになる.\n",
    "\n",
    "$$\n",
    "L = -\\sum_{p=1}^{p} y_p'*log(y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. テストプロセスの実装と説明\n",
    "\n",
    "### `x_test`と`y_test`を用いて学習したニューラルネットワークをテストするコードを実装せよ。（5点）\n",
    "\n",
    "特に、テスト結果として<font color=\"red\">精度（Accuracy）</font>を出力せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.2833448499679565\n",
      "Test accuracy: 0.5440999865531921\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上記のテストにおける精度の計算方法を数学的に説明せよ。（5点）\n",
    "\n",
    "以下の点に留意すること。\n",
    "\n",
    "- <font color=\"red\">Q=10000個のテスト用画像$\\left\\{ (\\mathbf{x}_q, y_q) \\right\\}_{q=1}^{Q}$とすること\n",
    "    \n",
    "    \n",
    "- $q$番目のテスト用画像に対する学習済みのニューラルネットワークの出力（**自分で定義する**)と、$q$番目のテスト用画像のラベル$y_q$を比較するはずである。学習済みのニューラルネットワークの出力は、上記の学習プロセスで定義したものとよく似てくるはずである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークの出力を$y$とする.\n",
    "\n",
    "$y$と$y_q$を比べたときに, 予測があっていれば$y=y_q$なのでこれを$T$とする.\n",
    "\n",
    "これ以外の事象を$N$とする.\n",
    "\n",
    "以上により精度は以下のような式になる.\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{T}{T+N}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">特典：精度が高い上位5名は、10点加点する！</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
